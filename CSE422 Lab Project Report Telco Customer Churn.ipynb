{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000},"id":"SWHhg1z8lven"},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/telco_customer_churn.csv'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-1-40c4bd7971a4\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 0\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Load the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 22\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/telco_customer_churn.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# ======================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u003e\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/telco_customer_churn.csv'"]}],"source":["# Import necessary libraries\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n","                            f1_score, confusion_matrix, roc_auc_score, roc_curve,\n","                            classification_report)\n","from imblearn.over_sampling import SMOTE\n","from imblearn.pipeline import make_pipeline as make_imb_pipeline\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Load the dataset\n","data = pd.read_csv('/content/telco_customer_churn.csv')\n","\n","# ======================\n","# 1. Dataset Description\n","# ======================\n","print(\"=\"*50)\n","print(\"1. DATASET DESCRIPTION\")\n","print(\"=\"*50)\n","print(\"\\nDataset Shape:\", data.shape)\n","print(\"\\nFirst 5 Rows:\")\n","print(data.head())\n","print(\"\\nData Types:\")\n","print(data.dtypes)\n","print(\"\\nMissing Values:\")\n","print(data.isnull().sum())\n","print(\"\\nData Description (Numerical Features):\")\n","print(data.describe())\n","\n","# Check for empty strings which might represent missing values\n","print(\"\\nChecking for empty strings in categorical columns:\")\n","for col in data.select_dtypes(include=['object']).columns:\n","    empty_count = (data[col] == '').sum()\n","    if empty_count \u003e 0:\n","        print(f\"{col}: {empty_count} empty values\")\n","\n","# Handle the missing value in TotalCharges (found as empty string)\n","# Changed inplace operation to avoid FutureWarning\n","data['TotalCharges'] = pd.to_numeric(data['TotalCharges'], errors='coerce')\n","print(\"\\nMissing Values after conversion:\")\n","print(data.isnull().sum())\n","\n","# Fill missing TotalCharges with 0 (likely new customers)\n","data['TotalCharges'] = data['TotalCharges'].fillna(0)\n","\n","# ======================\n","# 2. Correlation Analysis\n","# ======================\n","print(\"\\n\" + \"=\"*50)\n","print(\"2. CORRELATION ANALYSIS\")\n","print(\"=\"*50)\n","\n","# Convert target to numerical for correlation\n","data['Churn_numeric'] = data['Churn'].apply(lambda x: 1 if x == 'Yes' else 0)\n","\n","plt.figure(figsize=(15, 10))\n","numeric_cols = data.select_dtypes(include=['int64', 'float64']).columns\n","corr_matrix = data[numeric_cols].corr()\n","sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, fmt='.2f')\n","plt.title(\"Correlation Heatmap\")\n","plt.tight_layout()\n","plt.show()\n","\n","# Top correlations with Churn\n","print(\"\\nTop features correlated with Churn:\")\n","corr_with_churn = corr_matrix['Churn_numeric'].sort_values(ascending=False)\n","print(corr_with_churn)\n","\n","# ======================\n","# 3. Exploratory Data Analysis\n","# ======================\n","print(\"\\n\" + \"=\"*50)\n","print(\"3. EXPLORATORY DATA ANALYSIS\")\n","print(\"=\"*50)\n","\n","# 3.1 Target Variable Distribution\n","plt.figure(figsize=(12, 5))\n","plt.subplot(1, 2, 1)\n","sns.countplot(x='Churn', data=data)\n","plt.title(\"Distribution of Churn\")\n","\n","plt.subplot(1, 2, 2)\n","data['Churn'].value_counts().plot.pie(autopct='%1.1f%%')\n","plt.title(\"Churn Percentage\")\n","plt.ylabel(\"\")\n","plt.tight_layout()\n","plt.show()\n","\n","# 3.2 Categorical Features vs Churn\n","plt.figure(figsize=(18, 15))\n","\n","plt.subplot(3, 3, 1)\n","sns.countplot(x='gender', hue='Churn', data=data)\n","plt.title(\"Churn by Gender\")\n","\n","plt.subplot(3, 3, 2)\n","sns.countplot(x='SeniorCitizen', hue='Churn', data=data)\n","plt.title(\"Churn by Senior Citizen Status\")\n","\n","plt.subplot(3, 3, 3)\n","sns.countplot(x='Partner', hue='Churn', data=data)\n","plt.title(\"Churn by Partner Status\")\n","\n","plt.subplot(3, 3, 4)\n","sns.countplot(x='Dependents', hue='Churn', data=data)\n","plt.title(\"Churn by Dependents\")\n","\n","plt.subplot(3, 3, 5)\n","sns.countplot(x='PhoneService', hue='Churn', data=data)\n","plt.title(\"Churn by Phone Service\")\n","\n","plt.subplot(3, 3, 6)\n","sns.countplot(x='InternetService', hue='Churn', data=data)\n","plt.title(\"Churn by Internet Service\")\n","\n","plt.subplot(3, 3, 7)\n","sns.countplot(x='Contract', hue='Churn', data=data)\n","plt.title(\"Churn by Contract Type\")\n","\n","plt.subplot(3, 3, 8)\n","sns.countplot(x='PaperlessBilling', hue='Churn', data=data)\n","plt.title(\"Churn by Paperless Billing\")\n","\n","plt.subplot(3, 3, 9)\n","sns.countplot(x='PaymentMethod', hue='Churn', data=data)\n","plt.xticks(rotation=45)\n","plt.title(\"Churn by Payment Method\")\n","plt.tight_layout()\n","plt.show()\n","\n","# 3.3 Numerical Features vs Churn\n","plt.figure(figsize=(18, 10))\n","\n","plt.subplot(2, 3, 1)\n","sns.boxplot(x='Churn', y='tenure', data=data)\n","plt.title(\"Churn by Tenure\")\n","\n","plt.subplot(2, 3, 2)\n","sns.boxplot(x='Churn', y='MonthlyCharges', data=data)\n","plt.title(\"Churn by Monthly Charges\")\n","\n","plt.subplot(2, 3, 3)\n","sns.boxplot(x='Churn', y='TotalCharges', data=data)\n","plt.title(\"Churn by Total Charges\")\n","\n","# Create tenure groups\n","data['tenure_group'] = pd.cut(data['tenure'], bins=[0, 12, 24, 36, 48, 60, 72],\n","                            labels=['0-12', '12-24', '24-36', '36-48', '48-60', '60-72'])\n","\n","plt.subplot(2, 3, 4)\n","sns.countplot(x='tenure_group', hue='Churn', data=data)\n","plt.title(\"Churn by Tenure Groups\")\n","plt.xticks(rotation=45)\n","\n","plt.subplot(2, 3, 5)\n","sns.histplot(data=data, x='MonthlyCharges', hue='Churn', element='step', stat='density', common_norm=False)\n","plt.title(\"Monthly Charges Distribution by Churn\")\n","\n","plt.subplot(2, 3, 6)\n","sns.histplot(data=data, x='TotalCharges', hue='Churn', element='step', stat='density', common_norm=False)\n","plt.title(\"Total Charges Distribution by Churn\")\n","plt.tight_layout()\n","plt.show()\n","\n","# ======================\n","# 4. Data Preprocessing\n","# ======================\n","print(\"\\n\" + \"=\"*50)\n","print(\"4. DATA PREPROCESSING\")\n","print(\"=\"*50)\n","\n","# Drop unnecessary columns\n","data = data.drop(['customerID', 'Churn_numeric', 'tenure_group'], axis=1)\n","\n","# Separate features and target\n","X = data.drop('Churn', axis=1)\n","y = data['Churn']\n","\n","# Convert target to binary\n","y = y.map({'Yes': 1, 'No': 0})\n","\n","# Identify categorical and numerical columns\n","categorical_cols = X.select_dtypes(include=['object']).columns\n","numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns\n","\n","print(\"\\nCategorical Features:\", list(categorical_cols))\n","print(\"\\nNumerical Features:\", list(numeric_cols))\n","\n","# Create preprocessing pipeline\n","preprocessor = ColumnTransformer(\n","    transformers=[\n","        ('num', StandardScaler(), numeric_cols),\n","        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n","    ])\n","\n","# Split data into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.3, random_state=42, stratify=y)\n","\n","print(f\"\\nTraining set size: {X_train.shape[0]} samples\")\n","print(f\"Test set size: {X_test.shape[0]} samples\")\n","\n","# Check class distribution\n","print(\"\\nClass distribution in training set:\")\n","print(pd.Series(y_train).value_counts(normalize=True))\n","\n","# ======================\n","# 5. Model Training\n","# ======================\n","print(\"\\n\" + \"=\"*50)\n","print(\"5. MODEL TRAINING\")\n","print(\"=\"*50)\n","\n","# Function to train and evaluate models\n","def train_evaluate_model(model, model_name, X_train, y_train, X_test, y_test):\n","    print(f\"\\nTraining {model_name}...\")\n","    model.fit(X_train, y_train)\n","    y_pred = model.predict(X_test)\n","    y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n","\n","    # Calculate metrics\n","    accuracy = accuracy_score(y_test, y_pred)\n","    precision = precision_score(y_test, y_pred)\n","    recall = recall_score(y_test, y_pred)\n","    f1 = f1_score(y_test, y_pred)\n","    roc_auc = roc_auc_score(y_test, y_prob) if y_prob is not None else None\n","\n","    # Confusion matrix\n","    cm = confusion_matrix(y_test, y_pred)\n","\n","    # Classification report\n","    print(\"\\nClassification Report:\")\n","    print(classification_report(y_test, y_pred))\n","\n","    # Plot confusion matrix\n","    plt.figure(figsize=(6, 4))\n","    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n","                xticklabels=['No Churn', 'Churn'],\n","                yticklabels=['No Churn', 'Churn'])\n","    plt.title(f\"Confusion Matrix - {model_name}\")\n","    plt.ylabel('Actual')\n","    plt.xlabel('Predicted')\n","    plt.show()\n","\n","    # Plot ROC curve if probabilities are available\n","    if y_prob is not None:\n","        fpr, tpr, _ = roc_curve(y_test, y_prob)\n","        plt.figure(figsize=(6, 4))\n","        plt.plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc:.2f})')\n","        plt.plot([0, 1], [0, 1], 'k--')\n","        plt.xlabel('False Positive Rate')\n","        plt.ylabel('True Positive Rate')\n","        plt.title(f'ROC Curve - {model_name}')\n","        plt.legend()\n","        plt.show()\n","\n","    return {\n","        'model': model_name,\n","        'accuracy': accuracy,\n","        'precision': precision,\n","        'recall': recall,\n","        'f1': f1,\n","        'roc_auc': roc_auc\n","    }\n","\n","# Create a function to handle model training with SMOTE\n","def train_model_with_smote(model, params, X_train, y_train):\n","    # Create pipeline with SMOTE and the model\n","    pipeline = make_imb_pipeline(\n","        preprocessor,\n","        SMOTE(random_state=42),\n","        model\n","    )\n","\n","    # Grid search with cross-validation\n","    grid = GridSearchCV(\n","        pipeline,\n","        params,\n","        cv=StratifiedKFold(n_splits=5),\n","        scoring='f1',\n","        n_jobs=-1\n","    )\n","\n","    grid.fit(X_train, y_train)\n","    return grid.best_estimator_\n","\n","# Decision Tree\n","dt_params = {\n","    'decisiontreeclassifier__max_depth': [3, 5, 7, None],\n","    'decisiontreeclassifier__min_samples_split': [2, 5, 10],\n","    'decisiontreeclassifier__criterion': ['gini', 'entropy']\n","}\n","\n","best_dt = train_model_with_smote(\n","    DecisionTreeClassifier(random_state=42),\n","    dt_params,\n","    X_train,\n","    y_train\n",")\n","\n","dt_results = train_evaluate_model(best_dt, \"Decision Tree\",\n","                                X_train, y_train, X_test, y_test)\n","\n","# Logistic Regression\n","lr_params = {\n","    'logisticregression__C': [0.01, 0.1, 1, 10],\n","    'logisticregression__solver': ['liblinear', 'lbfgs'],\n","    'logisticregression__class_weight': ['balanced', None]\n","}\n","\n","best_lr = train_model_with_smote(\n","    LogisticRegression(random_state=42, max_iter=1000),\n","    lr_params,\n","    X_train,\n","    y_train\n",")\n","\n","lr_results = train_evaluate_model(best_lr, \"Logistic Regression\",\n","                                X_train, y_train, X_test, y_test)\n","\n","# Neural Network\n","nn_params = {\n","    'mlpclassifier__hidden_layer_sizes': [(50,), (100,), (50, 30)],\n","    'mlpclassifier__alpha': [0.0001, 0.001],\n","    'mlpclassifier__activation': ['relu', 'tanh'],\n","    'mlpclassifier__learning_rate_init': [0.001, 0.01]\n","}\n","\n","best_nn = train_model_with_smote(\n","    MLPClassifier(\n","        random_state=42,\n","        max_iter=1000,\n","        early_stopping=True,\n","        validation_fraction=0.2,\n","        n_iter_no_change=20,\n","        learning_rate='adaptive'\n","    ),\n","    nn_params,\n","    X_train,\n","    y_train\n",")\n","\n","nn_results = train_evaluate_model(best_nn, \"Neural Network\",\n","                                X_train, y_train, X_test, y_test)\n","\n","# ======================\n","# 6. Model Comparison\n","# ======================\n","print(\"\\n\" + \"=\"*50)\n","print(\"6. MODEL COMPARISON\")\n","print(\"=\"*50)\n","\n","# Gather all results\n","results = pd.DataFrame([dt_results, lr_results, nn_results])\n","\n","# Plot comparison metrics\n","plt.figure(figsize=(15, 10))\n","\n","plt.subplot(2, 2, 1)\n","sns.barplot(x='model', y='accuracy', data=results)\n","plt.title(\"Accuracy Comparison\")\n","plt.xticks(rotation=45)\n","plt.ylim(0, 1)\n","\n","plt.subplot(2, 2, 2)\n","sns.barplot(x='model', y='precision', data=results)\n","plt.title(\"Precision Comparison\")\n","plt.xticks(rotation=45)\n","plt.ylim(0, 1)\n","\n","plt.subplot(2, 2, 3)\n","sns.barplot(x='model', y='recall', data=results)\n","plt.title(\"Recall Comparison\")\n","plt.xticks(rotation=45)\n","plt.ylim(0, 1)\n","\n","plt.subplot(2, 2, 4)\n","sns.barplot(x='model', y='f1', data=results)\n","plt.title(\"F1 Score Comparison\")\n","plt.xticks(rotation=45)\n","plt.ylim(0, 1)\n","\n","plt.tight_layout()\n","plt.show()\n","\n","# ROC Curve comparison\n","plt.figure(figsize=(10, 8))\n","models = {\n","    'Decision Tree': best_dt,\n","    'Logistic Regression': best_lr,\n","    'Neural Network': best_nn\n","}\n","\n","for name, model in models.items():\n","    if hasattr(model, 'predict_proba'):\n","        y_prob = model.predict_proba(X_test)[:, 1]\n","        fpr, tpr, _ = roc_curve(y_test, y_prob)\n","        auc = roc_auc_score(y_test, y_prob)\n","        plt.plot(fpr, tpr, label=f'{name} (AUC = {auc:.2f})')\n","\n","plt.plot([0, 1], [0, 1], 'k--')\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('ROC Curve Comparison')\n","plt.legend()\n","plt.show()\n","\n","# Display all results\n","print(\"\\nModel Performance Comparison:\")\n","print(results)\n","\n","# Best Parameters\n","print(\"\\nBest Decision Tree Parameters:\")\n","print(best_dt.named_steps['decisiontreeclassifier'].get_params())\n","\n","print(\"\\nBest Logistic Regression Parameters:\")\n","print(best_lr.named_steps['logisticregression'].get_params())\n","\n","print(\"\\nBest Neural Network Parameters:\")\n","print(best_nn.named_steps['mlpclassifier'].get_params())\n","\n","# Feature Importance (Decision Tree)\n","if hasattr(best_dt.named_steps['decisiontreeclassifier'], 'feature_importances_'):\n","    # Get the fitted preprocessor from the pipeline\n","    fitted_preprocessor = best_dt.named_steps['columntransformer']\n","\n","    # Get feature names after one-hot encoding\n","    ohe = fitted_preprocessor.named_transformers_['cat']\n","    cat_features = ohe.get_feature_names_out(categorical_cols)\n","    all_features = np.concatenate([numeric_cols, cat_features])\n","\n","    # Get importance scores\n","    importances = best_dt.named_steps['decisiontreeclassifier'].feature_importances_\n","    indices = np.argsort(importances)[-10:]  # Top 10 features\n","\n","    plt.figure(figsize=(12, 8))\n","    plt.title(\"Top 10 Important Features (Decision Tree)\")\n","    plt.barh(range(10), importances[indices], align='center')\n","    plt.yticks(range(10), [all_features[i] for i in indices])\n","    plt.xlabel(\"Relative Importance\")\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"dsNIea8ZpotZ"},"source":["# New Section"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sA7q2IdKnnkm"},"outputs":[],"source":[]}],"metadata":{"colab":{"name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}